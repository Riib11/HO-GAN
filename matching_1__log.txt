Unable to init server: Could not connect: Connection refused
Unable to init server: Could not connect: Connection refused

(dcgan_custom.py:101991): Gdk-CRITICAL **: 15:03:41.776: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed

(dcgan_custom.py:101991): Gdk-CRITICAL **: 15:03:41.777: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed
Using TensorFlow backend.
  0%|          | 0/1000 [00:00<?, ?it/s] 99%|█████████▉| 994/1000 [00:00<00:00, 9932.41it/s]100%|██████████| 1000/1000 [00:00<00:00, 9895.87it/s]
WARNING:tensorflow:From /home/blancheh/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/blancheh/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
2019-05-09 15:03:42.341270: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-05-09 15:03:42.362546: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3393280000 Hz
2019-05-09 15:03:42.364432: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x28c7dc0 executing computations on platform Host. Devices:
2019-05-09 15:03:42.364450: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
WARNING:tensorflow:From /home/blancheh/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
loading images in directory: same
loaded 1 classes: ['same']
loaded 1000 images
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 14, 14, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 7, 7, 128)         204928    
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 7, 7, 128)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 4, 4, 256)         0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 4, 4, 512)         3277312   
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 512)         0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 4, 4, 512)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 8192)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 8193      
_________________________________________________________________
activation_1 (Activation)    (None, 1)                 0         
=================================================================
Total params: 4,311,553
Trainable params: 4,311,553
Non-trainable params: 0
_________________________________________________________________
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_2 (Dense)              (None, 12544)             1266944   
_________________________________________________________________
batch_normalization_1 (Batch (None, 12544)             50176     
_________________________________________________________________
activation_2 (Activation)    (None, 12544)             0         
_________________________________________________________________
reshape_1 (Reshape)          (None, 7, 7, 256)         0         
_________________________________________________________________
dropout_5 (Dropout)          (None, 7, 7, 256)         0         
_________________________________________________________________
up_sampling2d_1 (UpSampling2 (None, 14, 14, 256)       0         
_________________________________________________________________
conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       819328    
_________________________________________________________________
batch_normalization_2 (Batch (None, 14, 14, 128)       512       
_________________________________________________________________
activation_3 (Activation)    (None, 14, 14, 128)       0         
_________________________________________________________________
up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         
_________________________________________________________________
conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        204864    
_________________________________________________________________
batch_normalization_3 (Batch (None, 28, 28, 64)        256       
_________________________________________________________________
activation_4 (Activation)    (None, 28, 28, 64)        0         
_________________________________________________________________
conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        51232     
_________________________________________________________________
batch_normalization_4 (Batch (None, 28, 28, 32)        128       
_________________________________________________________________
activation_5 (Activation)    (None, 28, 28, 32)        0         
_________________________________________________________________
conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801       
_________________________________________________________________
activation_6 (Activation)    (None, 28, 28, 1)         0         
=================================================================
Total params: 2,394,241
Trainable params: 2,368,705
Non-trainable params: 25,536
_________________________________________________________________
0: [D loss: 0.694685, acc: 0.468750]  [A loss: 0.624779, acc: 1.000000]
1: [D loss: 0.756817, acc: 0.500000]  [A loss: 0.630229, acc: 0.968750]
2: [D loss: 0.787108, acc: 0.500000]  [A loss: 1.247326, acc: 0.000000]
3: [D loss: 0.601315, acc: 0.500000]  [A loss: 0.898877, acc: 0.031250]
4: [D loss: 0.931783, acc: 0.500000]  [A loss: 7.392964, acc: 0.000000]
5: [D loss: 1.317515, acc: 0.500000]  [A loss: 0.004173, acc: 1.000000]
6: [D loss: 1.315601, acc: 0.500000]  [A loss: 0.114906, acc: 1.000000]
7: [D loss: 0.864043, acc: 0.500000]  [A loss: 0.332910, acc: 1.000000]
8: [D loss: 0.704708, acc: 0.500000]  [A loss: 0.484678, acc: 1.000000]
9: [D loss: 0.678743, acc: 0.500000]  [A loss: 0.642823, acc: 0.843750]
10: [D loss: 0.694868, acc: 0.500000]  [A loss: 0.805294, acc: 0.156250]
11: [D loss: 0.747073, acc: 0.500000]  [A loss: 1.301755, acc: 0.000000]
12: [D loss: 0.726784, acc: 0.500000]  [A loss: 1.606625, acc: 0.000000]
13: [D loss: 0.767937, acc: 0.500000]  [A loss: 3.386776, acc: 0.000000]
14: [D loss: 0.811004, acc: 0.562500]  [A loss: 0.031136, acc: 1.000000]
15: [D loss: 1.857191, acc: 0.500000]  [A loss: 0.655916, acc: 0.625000]
16: [D loss: 0.681272, acc: 0.500000]  [A loss: 0.912125, acc: 0.062500]
17: [D loss: 0.726460, acc: 0.500000]  [A loss: 0.869363, acc: 0.218750]
18: [D loss: 0.716237, acc: 0.484375]  [A loss: 1.190992, acc: 0.000000]
19: [D loss: 0.816678, acc: 0.468750]  [A loss: 2.316277, acc: 0.000000]
20: [D loss: 0.768291, acc: 0.515625]  [A loss: 3.332286, acc: 0.000000]
21: [D loss: 0.684911, acc: 0.437500]  [A loss: 0.607808, acc: 0.750000]
22: [D loss: 1.117491, acc: 0.500000]  [A loss: 7.619594, acc: 0.000000]
23: [D loss: 2.228350, acc: 0.500000]  [A loss: 0.008868, acc: 1.000000]
24: [D loss: 2.349211, acc: 0.500000]  [A loss: 0.198228, acc: 1.000000]
25: [D loss: 1.002516, acc: 0.500000]  [A loss: 0.472748, acc: 0.968750]
26: [D loss: 0.797838, acc: 0.500000]  [A loss: 0.670108, acc: 0.687500]
27: [D loss: 0.706323, acc: 0.500000]  [A loss: 0.894867, acc: 0.062500]
28: [D loss: 0.740549, acc: 0.500000]  [A loss: 0.912998, acc: 0.125000]
29: [D loss: 0.728573, acc: 0.515625]  [A loss: 1.041666, acc: 0.125000]
30: [D loss: 0.759888, acc: 0.484375]  [A loss: 1.449995, acc: 0.000000]
31: [D loss: 0.672622, acc: 0.546875]  [A loss: 1.411637, acc: 0.031250]
32: [D loss: 0.817356, acc: 0.500000]  [A loss: 2.253152, acc: 0.000000]
33: [D loss: 0.703462, acc: 0.437500]  [A loss: 0.428599, acc: 0.968750]
34: [D loss: 1.093946, acc: 0.500000]  [A loss: 3.608676, acc: 0.000000]
35: [D loss: 1.123299, acc: 0.531250]  [A loss: 0.022432, acc: 1.000000]
36: [D loss: 2.105622, acc: 0.500000]  [A loss: 0.363195, acc: 1.000000]
37: [D loss: 0.889846, acc: 0.500000]  [A loss: 0.930199, acc: 0.125000]
38: [D loss: 0.712624, acc: 0.484375]  [A loss: 0.519269, acc: 0.875000]
39: [D loss: 0.800007, acc: 0.500000]  [A loss: 1.092248, acc: 0.031250]
40: [D loss: 0.638412, acc: 0.500000]  [A loss: 0.593455, acc: 0.781250]
41: [D loss: 0.744478, acc: 0.484375]  [A loss: 1.103834, acc: 0.093750]
42: [D loss: 0.671032, acc: 0.500000]  [A loss: 0.465355, acc: 0.968750]
43: [D loss: 0.965479, acc: 0.500000]  [A loss: 2.203596, acc: 0.000000]
44: [D loss: 0.717609, acc: 0.546875]  [A loss: 0.139038, acc: 1.000000]
45: [D loss: 1.260288, acc: 0.500000]  [A loss: 1.029664, acc: 0.062500]
46: [D loss: 0.650246, acc: 0.484375]  [A loss: 0.515062, acc: 0.937500]
47: [D loss: 0.820460, acc: 0.500000]  [A loss: 1.324961, acc: 0.031250]
48: [D loss: 0.605289, acc: 0.531250]  [A loss: 0.458917, acc: 0.937500]
49: [D loss: 0.901890, acc: 0.500000]  [A loss: 2.266658, acc: 0.000000]
50: [D loss: 0.756485, acc: 0.437500]  [A loss: 0.106626, acc: 1.000000]
51: [D loss: 1.367056, acc: 0.500000]  [A loss: 0.901839, acc: 0.218750]
52: [D loss: 0.777466, acc: 0.484375]  [A loss: 1.505793, acc: 0.000000]
53: [D loss: 0.818300, acc: 0.406250]  [A loss: 0.764278, acc: 0.531250]
54: [D loss: 0.999258, acc: 0.500000]  [A loss: 3.468454, acc: 0.000000]
55: [D loss: 1.001831, acc: 0.531250]  [A loss: 0.026259, acc: 1.000000]
56: [D loss: 2.622811, acc: 0.500000]  [A loss: 0.526892, acc: 0.906250]
57: [D loss: 0.840752, acc: 0.500000]  [A loss: 1.017073, acc: 0.031250]
58: [D loss: 0.680003, acc: 0.515625]  [A loss: 0.778327, acc: 0.468750]
59: [D loss: 0.784262, acc: 0.500000]  [A loss: 1.014560, acc: 0.062500]
60: [D loss: 0.756212, acc: 0.515625]  [A loss: 1.637382, acc: 0.000000]
61: [D loss: 0.722970, acc: 0.484375]  [A loss: 0.487838, acc: 0.906250]
62: [D loss: 1.004012, acc: 0.484375]  [A loss: 2.713378, acc: 0.000000]
63: [D loss: 0.698307, acc: 0.500000]  [A loss: 0.411712, acc: 0.937500]
64: [D loss: 1.240365, acc: 0.500000]  [A loss: 3.818902, acc: 0.000000]
65: [D loss: 1.044529, acc: 0.593750]  [A loss: 0.022169, acc: 1.000000]
66: [D loss: 2.886819, acc: 0.500000]  [A loss: 0.651309, acc: 0.718750]
67: [D loss: 0.826776, acc: 0.515625]  [A loss: 1.002647, acc: 0.062500]
68: [D loss: 0.650436, acc: 0.531250]  [A loss: 0.573115, acc: 0.781250]
69: [D loss: 0.884132, acc: 0.500000]  [A loss: 1.528536, acc: 0.000000]
70: [D loss: 0.708270, acc: 0.531250]  [A loss: 0.317153, acc: 1.000000]
71: [D loss: 1.187889, acc: 0.500000]  [A loss: 1.782242, acc: 0.000000]
72: [D loss: 0.753035, acc: 0.546875]  [A loss: 0.301987, acc: 1.000000]
73: [D loss: 1.296829, acc: 0.500000]  [A loss: 2.389159, acc: 0.000000]
74: [D loss: 0.809871, acc: 0.531250]  [A loss: 0.107333, acc: 1.000000]
75: [D loss: 1.820740, acc: 0.500000]  [A loss: 1.124299, acc: 0.093750]
76: [D loss: 0.715828, acc: 0.484375]  [A loss: 0.883394, acc: 0.250000]
77: [D loss: 0.891401, acc: 0.515625]  [A loss: 1.834910, acc: 0.000000]
78: [D loss: 0.696634, acc: 0.515625]  [A loss: 0.350885, acc: 0.968750]
79: [D loss: 1.350225, acc: 0.500000]  [A loss: 2.268724, acc: 0.000000]
80: [D loss: 0.730222, acc: 0.562500]  [A loss: 0.103357, acc: 1.000000]
81: [D loss: 2.170263, acc: 0.500000]  [A loss: 0.991517, acc: 0.125000]
82: [D loss: 0.805528, acc: 0.500000]  [A loss: 1.310649, acc: 0.093750]
83: [D loss: 0.858216, acc: 0.484375]  [A loss: 0.878026, acc: 0.343750]
84: [D loss: 0.866599, acc: 0.531250]  [A loss: 1.633058, acc: 0.031250]
85: [D loss: 0.676931, acc: 0.531250]  [A loss: 0.813772, acc: 0.437500]
86: [D loss: 0.945087, acc: 0.500000]  [A loss: 1.970446, acc: 0.000000]
87: [D loss: 0.607450, acc: 0.562500]  [A loss: 1.354732, acc: 0.031250]
88: [D loss: 0.838735, acc: 0.515625]  [A loss: 1.667422, acc: 0.031250]
89: [D loss: 0.858221, acc: 0.515625]  [A loss: 3.061118, acc: 0.000000]
90: [D loss: 0.968259, acc: 0.593750]  [A loss: 0.069229, acc: 1.000000]
91: [D loss: 2.173034, acc: 0.500000]  [A loss: 1.937344, acc: 0.000000]
92: [D loss: 0.833544, acc: 0.515625]  [A loss: 0.794977, acc: 0.437500]
93: [D loss: 1.027676, acc: 0.531250]  [A loss: 4.229624, acc: 0.000000]
94: [D loss: 2.486134, acc: 0.453125]  [A loss: 0.007455, acc: 1.000000]
95: [D loss: 5.003504, acc: 0.500000]  [A loss: 0.134541, acc: 1.000000]
96: [D loss: 1.979617, acc: 0.500000]  [A loss: 0.356738, acc: 1.000000]
97: [D loss: 1.045458, acc: 0.500000]  [A loss: 0.752468, acc: 0.406250]
98: [D loss: 0.691320, acc: 0.515625]  [A loss: 0.651705, acc: 0.656250]
99: [D loss: 0.761897, acc: 0.468750]  [A loss: 0.615242, acc: 0.656250]
100: [D loss: 0.757234, acc: 0.500000]  [A loss: 0.705385, acc: 0.437500]
101: [D loss: 0.732838, acc: 0.484375]  [A loss: 0.600932, acc: 0.718750]
102: [D loss: 0.892768, acc: 0.484375]  [A loss: 0.771986, acc: 0.312500]
103: [D loss: 0.706364, acc: 0.515625]  [A loss: 0.854647, acc: 0.250000]
104: [D loss: 0.692673, acc: 0.453125]  [A loss: 0.491728, acc: 0.906250]
105: [D loss: 1.054032, acc: 0.500000]  [A loss: 1.350417, acc: 0.031250]
106: [D loss: 0.637927, acc: 0.515625]  [A loss: 0.752420, acc: 0.406250]
107: [D loss: 0.738105, acc: 0.484375]  [A loss: 0.662342, acc: 0.687500]
108: [D loss: 0.883104, acc: 0.500000]  [A loss: 1.345843, acc: 0.031250]
109: [D loss: 0.655974, acc: 0.500000]  [A loss: 0.335350, acc: 1.000000]
110: [D loss: 1.321938, acc: 0.500000]  [A loss: 1.874012, acc: 0.000000]
111: [D loss: 0.809716, acc: 0.515625]  [A loss: 0.634773, acc: 0.625000]
112: [D loss: 0.964866, acc: 0.515625]  [A loss: 1.642683, acc: 0.031250]
113: [D loss: 0.732517, acc: 0.593750]  [A loss: 0.440365, acc: 0.906250]
114: [D loss: 0.950906, acc: 0.500000]  [A loss: 1.539015, acc: 0.031250]
115: [D loss: 0.799860, acc: 0.484375]  [A loss: 0.876305, acc: 0.281250]
116: [D loss: 0.837959, acc: 0.515625]  [A loss: 1.005908, acc: 0.187500]
117: [D loss: 1.114573, acc: 0.484375]  [A loss: 1.364254, acc: 0.000000]
118: [D loss: 0.919344, acc: 0.515625]  [A loss: 2.100323, acc: 0.000000]
119: [D loss: 0.915728, acc: 0.500000]  [A loss: 0.146661, acc: 1.000000]
120: [D loss: 2.165059, acc: 0.500000]  [A loss: 1.640102, acc: 0.000000]
121: [D loss: 0.735857, acc: 0.578125]  [A loss: 0.314759, acc: 1.000000]
122: [D loss: 1.598089, acc: 0.500000]  [A loss: 2.280429, acc: 0.000000]
123: [D loss: 1.279758, acc: 0.437500]  [A loss: 0.049954, acc: 1.000000]
124: [D loss: 3.017629, acc: 0.500000]  [A loss: 0.310986, acc: 1.000000]
125: [D loss: 1.312723, acc: 0.500000]  [A loss: 0.772884, acc: 0.406250]
126: [D loss: 0.703171, acc: 0.531250]  [A loss: 0.625013, acc: 0.656250]
127: [D loss: 0.781025, acc: 0.500000]  [A loss: 0.807690, acc: 0.156250]
128: [D loss: 0.624257, acc: 0.515625]  [A loss: 0.587621, acc: 0.812500]
129: [D loss: 0.748111, acc: 0.500000]  [A loss: 0.884265, acc: 0.250000]
130: [D loss: 0.596589, acc: 0.546875]  [A loss: 0.899289, acc: 0.156250]
131: [D loss: 0.645872, acc: 0.500000]  [A loss: 0.949218, acc: 0.156250]
132: [D loss: 0.745101, acc: 0.453125]  [A loss: 0.603665, acc: 0.718750]
133: [D loss: 0.900007, acc: 0.484375]  [A loss: 1.203629, acc: 0.031250]
134: [D loss: 0.725958, acc: 0.437500]  [A loss: 0.369288, acc: 1.000000]
135: [D loss: 1.087029, acc: 0.500000]  [A loss: 1.120036, acc: 0.031250]
136: [D loss: 0.669722, acc: 0.531250]  [A loss: 0.797557, acc: 0.406250]
137: [D loss: 0.932224, acc: 0.515625]  [A loss: 1.982143, acc: 0.000000]
138: [D loss: 0.901843, acc: 0.421875]  [A loss: 0.095077, acc: 1.000000]
139: [D loss: 2.190647, acc: 0.500000]  [A loss: 0.623859, acc: 0.656250]
140: [D loss: 0.891138, acc: 0.468750]  [A loss: 1.037141, acc: 0.031250]
141: [D loss: 0.738863, acc: 0.468750]  [A loss: 0.998159, acc: 0.187500]
142: [D loss: 0.701899, acc: 0.500000]  [A loss: 0.806824, acc: 0.375000]
143: [D loss: 0.821852, acc: 0.500000]  [A loss: 1.367887, acc: 0.031250]
144: [D loss: 0.622314, acc: 0.562500]  [A loss: 0.564434, acc: 0.656250]
145: [D loss: 0.969364, acc: 0.500000]  [A loss: 1.630405, acc: 0.000000]
146: [D loss: 0.791273, acc: 0.484375]  [A loss: 0.340403, acc: 1.000000]
147: [D loss: 1.242855, acc: 0.500000]  [A loss: 1.657769, acc: 0.000000]
148: [D loss: 0.934225, acc: 0.437500]  [A loss: 0.143215, acc: 1.000000]
149: [D loss: 1.631399, acc: 0.500000]  [A loss: 1.093086, acc: 0.093750]
150: [D loss: 0.776417, acc: 0.500000]  [A loss: 1.359514, acc: 0.031250]
151: [D loss: 0.813578, acc: 0.484375]  [A loss: 1.037630, acc: 0.250000]
152: [D loss: 0.902214, acc: 0.484375]  [A loss: 1.833090, acc: 0.000000]
153: [D loss: 0.703385, acc: 0.609375]  [A loss: 0.512293, acc: 0.750000]
154: [D loss: 1.133317, acc: 0.515625]  [A loss: 3.409065, acc: 0.000000]
155: [D loss: 0.747929, acc: 0.531250]  [A loss: 0.083982, acc: 1.000000]
156: [D loss: 1.899689, acc: 0.500000]  [A loss: 1.154295, acc: 0.125000]
157: [D loss: 0.839335, acc: 0.468750]  [A loss: 1.741719, acc: 0.000000]
158: [D loss: 0.737239, acc: 0.500000]  [A loss: 0.513175, acc: 0.843750]
159: [D loss: 1.077421, acc: 0.500000]  [A loss: 2.981873, acc: 0.000000]
160: [D loss: 1.098163, acc: 0.500000]  [A loss: 0.014806, acc: 1.000000]
161: [D loss: 2.961178, acc: 0.500000]  [A loss: 0.225229, acc: 1.000000]
162: [D loss: 1.279095, acc: 0.500000]  [A loss: 0.805481, acc: 0.406250]
163: [D loss: 0.856523, acc: 0.500000]  [A loss: 0.719902, acc: 0.437500]
164: [D loss: 0.732753, acc: 0.484375]  [A loss: 0.937667, acc: 0.187500]
165: [D loss: 0.746554, acc: 0.531250]  [A loss: 0.880078, acc: 0.312500]
166: [D loss: 0.736902, acc: 0.515625]  [A loss: 1.100992, acc: 0.093750]
167: [D loss: 0.691645, acc: 0.531250]  [A loss: 0.833086, acc: 0.375000]
168: [D loss: 0.702778, acc: 0.500000]  [A loss: 1.275354, acc: 0.000000]
169: [D loss: 0.816813, acc: 0.515625]  [A loss: 1.130115, acc: 0.093750]
170: [D loss: 0.857314, acc: 0.515625]  [A loss: 2.060997, acc: 0.000000]
171: [D loss: 0.612733, acc: 0.531250]  [A loss: 0.248175, acc: 1.000000]
172: [D loss: 1.446607, acc: 0.500000]  [A loss: 2.661140, acc: 0.000000]
173: [D loss: 0.806694, acc: 0.578125]  [A loss: 0.023108, acc: 1.000000]
174: [D loss: 2.272818, acc: 0.500000]  [A loss: 0.247490, acc: 1.000000]
175: [D loss: 1.004371, acc: 0.500000]  [A loss: 0.770706, acc: 0.343750]
176: [D loss: 0.653751, acc: 0.515625]  [A loss: 1.189010, acc: 0.125000]
177: [D loss: 0.636449, acc: 0.593750]  [A loss: 0.510763, acc: 0.843750]
178: [D loss: 0.851505, acc: 0.500000]  [A loss: 1.470217, acc: 0.000000]
179: [D loss: 0.703198, acc: 0.578125]  [A loss: 0.464646, acc: 0.812500]
180: [D loss: 0.965097, acc: 0.484375]  [A loss: 1.356138, acc: 0.062500]
181: [D loss: 0.757600, acc: 0.500000]  [A loss: 1.904083, acc: 0.031250]
182: [D loss: 0.693127, acc: 0.531250]  [A loss: 0.118653, acc: 1.000000]
183: [D loss: 1.471091, acc: 0.500000]  [A loss: 1.109556, acc: 0.093750]
184: [D loss: 0.834828, acc: 0.484375]  [A loss: 0.884073, acc: 0.281250]
185: [D loss: 0.720653, acc: 0.500000]  [A loss: 0.285537, acc: 1.000000]
186: [D loss: 1.175661, acc: 0.500000]  [A loss: 1.899008, acc: 0.000000]
187: [D loss: 0.776519, acc: 0.500000]  [A loss: 0.674373, acc: 0.531250]
188: [D loss: 0.994007, acc: 0.500000]  [A loss: 2.718982, acc: 0.000000]
189: [D loss: 0.870970, acc: 0.515625]  [A loss: 0.014426, acc: 1.000000]
190: [D loss: 2.230002, acc: 0.500000]  [A loss: 0.197790, acc: 1.000000]
191: [D loss: 1.080850, acc: 0.500000]  [A loss: 0.657876, acc: 0.468750]
192: [D loss: 0.719077, acc: 0.500000]  [A loss: 0.612066, acc: 0.687500]
193: [D loss: 0.709256, acc: 0.500000]  [A loss: 0.944118, acc: 0.187500]
194: [D loss: 0.880794, acc: 0.484375]  [A loss: 0.127565, acc: 1.000000]
195: [D loss: 1.334049, acc: 0.500000]  [A loss: 0.491126, acc: 0.968750]
196: [D loss: 0.831081, acc: 0.500000]  [A loss: 1.046067, acc: 0.125000]
197: [D loss: 0.653094, acc: 0.546875]  [A loss: 0.579981, acc: 0.781250]
198: [D loss: 0.747973, acc: 0.500000]  [A loss: 1.360275, acc: 0.156250]
199: [D loss: 0.791410, acc: 0.484375]  [A loss: 0.283064, acc: 0.937500]
200: [D loss: 1.220289, acc: 0.500000]  [A loss: 1.464402, acc: 0.000000]
201: [D loss: 0.579758, acc: 0.546875]  [A loss: 0.934110, acc: 0.156250]
202: [D loss: 0.629797, acc: 0.578125]  [A loss: 0.270258, acc: 0.968750]
203: [D loss: 1.150666, acc: 0.500000]  [A loss: 1.037054, acc: 0.125000]
204: [D loss: 0.656900, acc: 0.500000]  [A loss: 0.629020, acc: 0.656250]
205: [D loss: 0.912353, acc: 0.500000]  [A loss: 1.601549, acc: 0.000000]
206: [D loss: 0.708683, acc: 0.546875]  [A loss: 1.772125, acc: 0.031250]
207: [D loss: 0.751076, acc: 0.515625]  [A loss: 0.144649, acc: 1.000000]
208: [D loss: 1.266453, acc: 0.500000]  [A loss: 1.642077, acc: 0.031250]
209: [D loss: 0.790357, acc: 0.468750]  [A loss: 0.649234, acc: 0.718750]
210: [D loss: 0.900599, acc: 0.515625]  [A loss: 2.500358, acc: 0.000000]
211: [D loss: 0.749418, acc: 0.500000]  [A loss: 0.248613, acc: 0.968750]
212: [D loss: 1.164894, acc: 0.500000]  [A loss: 2.555072, acc: 0.000000]
213: [D loss: 0.936108, acc: 0.484375]  [A loss: 0.025406, acc: 1.000000]
214: [D loss: 2.391433, acc: 0.500000]  [A loss: 0.340562, acc: 0.937500]
215: [D loss: 0.970113, acc: 0.500000]  [A loss: 1.263513, acc: 0.156250]
216: [D loss: 0.820379, acc: 0.484375]  [A loss: 0.676836, acc: 0.562500]
217: [D loss: 1.080876, acc: 0.500000]  [A loss: 1.928135, acc: 0.062500]
218: [D loss: 0.806974, acc: 0.484375]  [A loss: 0.076852, acc: 1.000000]
219: [D loss: 1.780052, acc: 0.500000]  [A loss: 0.815656, acc: 0.468750]
220: [D loss: 0.783132, acc: 0.515625]  [A loss: 1.636145, acc: 0.000000]
221: [D loss: 0.845672, acc: 0.437500]  [A loss: 0.095819, acc: 1.000000]
222: [D loss: 1.760591, acc: 0.500000]  [A loss: 1.159643, acc: 0.156250]
223: [D loss: 0.820643, acc: 0.500000]  [A loss: 0.241539, acc: 1.000000]
224: [D loss: 1.166750, acc: 0.500000]  [A loss: 1.238176, acc: 0.218750]
225: [D loss: 0.963293, acc: 0.453125]  [A loss: 0.380800, acc: 0.968750]
226: [D loss: 0.984445, acc: 0.484375]  [A loss: 0.933078, acc: 0.250000]
227: [D loss: 0.886612, acc: 0.531250]  [A loss: 1.640170, acc: 0.000000]
228: [D loss: 0.813100, acc: 0.531250]  [A loss: 1.358823, acc: 0.062500]
229: [D loss: 0.935843, acc: 0.484375]  [A loss: 1.542017, acc: 0.093750]
230: [D loss: 0.728467, acc: 0.531250]  [A loss: 0.230128, acc: 0.968750]
231: [D loss: 1.391878, acc: 0.500000]  [A loss: 2.846075, acc: 0.000000]
232: [D loss: 0.962184, acc: 0.515625]  [A loss: 0.038848, acc: 1.000000]
233: [D loss: 1.798700, acc: 0.500000]  [A loss: 0.625694, acc: 0.687500]
234: [D loss: 0.972725, acc: 0.500000]  [A loss: 1.548310, acc: 0.062500]
235: [D loss: 0.791856, acc: 0.484375]  [A loss: 0.265387, acc: 0.968750]
236: [D loss: 1.300001, acc: 0.500000]  [A loss: 1.662875, acc: 0.031250]
237: [D loss: 0.694090, acc: 0.515625]  [A loss: 0.114393, acc: 1.000000]
238: [D loss: 1.559528, acc: 0.500000]  [A loss: 1.950177, acc: 0.062500]
239: [D loss: 0.705581, acc: 0.531250]  [A loss: 1.490790, acc: 0.062500]
240: [D loss: 0.619250, acc: 0.546875]  [A loss: 1.464884, acc: 0.031250]
241: [D loss: 1.088005, acc: 0.515625]  [A loss: 0.090820, acc: 1.000000]
242: [D loss: 1.622563, acc: 0.500000]  [A loss: 1.016648, acc: 0.218750]
243: [D loss: 0.725313, acc: 0.515625]  [A loss: 1.210588, acc: 0.156250]
244: [D loss: 0.777064, acc: 0.515625]  [A loss: 0.270530, acc: 1.000000]
245: [D loss: 1.174805, acc: 0.500000]  [A loss: 1.792021, acc: 0.000000]
246: [D loss: 0.785378, acc: 0.468750]  [A loss: 0.328690, acc: 0.906250]
247: [D loss: 0.993048, acc: 0.500000]  [A loss: 1.965383, acc: 0.031250]
248: [D loss: 0.606695, acc: 0.546875]  [A loss: 0.740277, acc: 0.562500]
249: [D loss: 0.776730, acc: 0.531250]  [A loss: 0.844034, acc: 0.406250]
250: [D loss: 0.750741, acc: 0.484375]  [A loss: 0.459418, acc: 0.843750]
251: [D loss: 1.114341, acc: 0.500000]  [A loss: 2.792041, acc: 0.000000]
252: [D loss: 0.788880, acc: 0.468750]  [A loss: 0.357803, acc: 0.937500]
253: [D loss: 1.245866, acc: 0.500000]  [A loss: 3.507385, acc: 0.000000]
254: [D loss: 1.199807, acc: 0.515625]  [A loss: 0.007545, acc: 1.000000]
255: [D loss: 2.392807, acc: 0.500000]  [A loss: 0.129984, acc: 1.000000]
256: [D loss: 1.195477, acc: 0.500000]  [A loss: 0.521585, acc: 0.718750]
257: [D loss: 0.778986, acc: 0.484375]  [A loss: 0.838344, acc: 0.343750]
258: [D loss: 0.769507, acc: 0.468750]  [A loss: 0.471574, acc: 0.843750]
259: [D loss: 0.940598, acc: 0.515625]  [A loss: 1.620505, acc: 0.031250]
260: [D loss: 0.780617, acc: 0.484375]  [A loss: 0.267539, acc: 1.000000]
261: [D loss: 0.944409, acc: 0.500000]  [A loss: 1.089835, acc: 0.187500]
262: [D loss: 0.694552, acc: 0.484375]  [A loss: 0.356751, acc: 0.937500]
263: [D loss: 0.960517, acc: 0.500000]  [A loss: 1.476704, acc: 0.031250]
264: [D loss: 0.720014, acc: 0.546875]  [A loss: 0.218905, acc: 1.000000]
265: [D loss: 1.310323, acc: 0.500000]  [A loss: 1.279297, acc: 0.031250]